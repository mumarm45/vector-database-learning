import os
from dotenv import load_dotenv
from typing import List, Dict, Any
from anthropic import Anthropic
from modules.create_basic import perform_similarity_search
from modules.create_chroma import initiate_food_collection

load_dotenv()

# Initialize Anthropic client
client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
MODEL = os.getenv("LLM_MODEL", "claude-3-haiku-20240307")


def generate_response(prompt: str) -> str:
    """Generate response using Anthropic Claude"""
    message = client.messages.create(
        model=MODEL,
        max_tokens=1000,
        messages=[{"role": "user", "content": prompt}]
    )
    return message.content[0].text

def prepare_context_for_llm(query: str, search_results: List[Dict]) -> str:
    """Prepare structured context from search results for LLM"""
    if not search_results:
        return "No relevant food items found in the database."
    
    context_parts = []
    context_parts.append("Based on your query, here are the most relevant food options from our database:")
    context_parts.append("")
    
    for i, result in enumerate(search_results[:3], 1):
        food_context = []
        food_context.append(f"Option {i}: {result['food_name']}")
        food_context.append(f"  - Description: {result['food_description']}")
        food_context.append(f"  - Cuisine: {result['cuisine_type']}")
        food_context.append(f"  - Calories: {result['food_calories_per_serving']} per serving")
        
        if result.get('food_ingredients'):
            ingredients = result['food_ingredients']
            if isinstance(ingredients, list):
                food_context.append(f"  - Key ingredients: {', '.join(ingredients[:5])}")
            else:
                food_context.append(f"  - Key ingredients: {ingredients}")
        
        if result.get('food_health_benefits'):
            food_context.append(f"  - Health benefits: {result['food_health_benefits']}")
        
        if result.get('cooking_method'):
            food_context.append(f"  - Cooking method: {result['cooking_method']}")
        
        if result.get('taste_profile'):
            food_context.append(f"  - Taste profile: {result['taste_profile']}")
        
        food_context.append(f"  - Similarity score: {result['similarity_score']*100:.1f}%")
        food_context.append("")
        
        context_parts.extend(food_context)
    
    return "\n".join(context_parts)

def generate_llm_rag_response(query: str, search_results: List[Dict]) -> str:
    """Generate response using Anthropic Claude with retrieved context"""
    try:
        context = prepare_context_for_llm(query, search_results)
        
        prompt = f'''You are a helpful food recommendation assistant. A user is asking for food recommendations, and I've retrieved relevant options from a food database.

    User Query: "{query}"

    Retrieved Food Information:
    {context}

    Please provide a helpful, short response that:
    1. Recommends 2-3 specific food items from the retrieved options
    2. Explains why these recommendations match their request
    3. Includes relevant details like cuisine type or calories
    4. Uses a friendly, conversational tone
    5. Keeps the response concise

    Response:'''

        return generate_response(prompt)
            
    except Exception as e:
        print(f"âŒ LLM Error: {e}")
        return generate_fallback_response(query, search_results)

def generate_fallback_response(query: str, search_results: List[Dict]) -> str:
    """Generate fallback response when LLM fails"""
    if not search_results:
        return "I couldn't find any food items matching your request. Try describing what you're in the mood for with different words!"
    
    top_result = search_results[0]
    response_parts = []
    
    response_parts.append(f"Based on your request for '{query}', I'd recommend {top_result['food_name']}.")
    response_parts.append(f"It's a {top_result['cuisine_type']} dish with {top_result['food_calories_per_serving']} calories per serving.")
    
    if len(search_results) > 1:
        second_choice = search_results[1]
        response_parts.append(f"Another great option would be {second_choice['food_name']}.")
    
    return " ".join(response_parts)

def start_chat(collection):
    """RAG-powered conversational food chatbot with Anthropic Claude"""
    print("\n" + "="*70)
    print("ğŸ¤– RAG FOOD RECOMMENDATION CHATBOT")
    print("   Powered by Anthropic Claude & ChromaDB")
    print("="*70)
    print("ğŸ’¬ Ask me about food recommendations using natural language!")
    print("\nExample queries:")
    print("  â€¢ 'I want something spicy and healthy for dinner'")
    print("  â€¢ 'What Italian dishes do you recommend under 400 calories?'")
    print("  â€¢ 'I'm craving comfort food for a cold evening'")
    print("  â€¢ 'Suggest some protein-rich breakfast options'")
    print("\nCommands:")
    print("  â€¢ 'help' - Show detailed help menu")
    print("  â€¢ 'compare' - Compare recommendations for two different queries")
    print("  â€¢ 'quit' - Exit the chatbot")
    print("-" * 70)
    
    conversation_history = []
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ You: ").strip()
            
            if not user_input:
                print("ğŸ¤– Bot: Please tell me what kind of food you're looking for!")
                continue
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ¤– Bot: Thank you for using the Enhanced RAG Food Chatbot!")
                print("      Hope you found some delicious recommendations! ğŸ‘‹")
                break
            
            elif user_input.lower() in ['help', 'h']:
                show_help()
            
            elif user_input.lower() in ['compare']:
                handle_comparison_mode(collection)
            
            else:
                # Process the food query with RAG
                handle_rag_query(collection, user_input, conversation_history)
                conversation_history.append(user_input)
                
                # Keep conversation history manageable
                if len(conversation_history) > 5:
                    conversation_history = conversation_history[-3:]
                
        except KeyboardInterrupt:
            print("\n\nğŸ¤– Bot: Goodbye! Hope you find something delicious! ğŸ‘‹")
            break
        except Exception as e:
            print(f"âŒ Bot: Sorry, I encountered an error: {e}")

def handle_rag_query(collection, query: str, conversation_history: List[str]):
    """Handle user query with RAG approach using Anthropic Claude"""
    print(f"\nğŸ” Searching vector database for: '{query}'...")
    
    # Perform similarity search with more results for better context
    search_results = perform_similarity_search(collection, query, 3)
    
    if not search_results:
        print("ğŸ¤– Bot: I couldn't find any food items matching your request.")
        print("      Try describing what you're in the mood for with different words!")
        return
    
    print(f"âœ… Found {len(search_results)} relevant matches")
    print("ğŸ§  Generating AI-powered response...")
    
    # Generate RAG response using Anthropic Claude
    ai_response = generate_llm_rag_response(query, search_results)
    
    print(f"\nğŸ¤– Bot: {ai_response}")
    
    # Show detailed results for reference
    print(f"\nğŸ“Š Search Results Details:")
    print("-" * 45)
    for i, result in enumerate(search_results[:3], 1):
        print(f"{i}. ğŸ½ï¸  {result['food_name']}")
        print(f"   ğŸ“ {result['cuisine_type']} | ğŸ”¥ {result['food_calories_per_serving']} cal | ğŸ“ˆ {result['similarity_score']*100:.1f}% match")
        if i < 3:
            print()

def handle_comparison_mode(collection):
    """Enhanced comparison between two food queries using LLM"""
    print("\nğŸ”„ ENHANCED COMPARISON MODE")
    print("   Powered by AI Analysis")
    print("-" * 35)
    
    query1 = input("Enter first food query: ").strip()
    query2 = input("Enter second food query: ").strip()
    
    if not query1 or not query2:
        print("âŒ Please enter both queries for comparison")
        return
    
    print(f"\nğŸ” Analyzing '{query1}' vs '{query2}' with AI...")
    
    # Get results for both queries
    results1 = perform_similarity_search(collection, query1, 3)
    results2 = perform_similarity_search(collection, query2, 3)
    
    # Generate AI-powered comparison
    comparison_response = generate_llm_comparison(query1, query2, results1, results2)
    
    print(f"\nğŸ¤– AI Analysis: {comparison_response}")
    
    # Show side-by-side results
    print(f"\nğŸ“Š DETAILED COMPARISON")
    print("=" * 60)
    print(f"{'Query 1: ' + query1[:20] + '...' if len(query1) > 20 else 'Query 1: ' + query1:<30} | {'Query 2: ' + query2[:20] + '...' if len(query2) > 20 else 'Query 2: ' + query2}")
    print("-" * 60)
    
    max_results = max(len(results1), len(results2))
    for i in range(min(max_results, 3)):
        left = f"{results1[i]['food_name']} ({results1[i]['similarity_score']*100:.0f}%)" if i < len(results1) else "---"
        right = f"{results2[i]['food_name']} ({results2[i]['similarity_score']*100:.0f}%)" if i < len(results2) else "---"
        print(f"{left[:30]:<30} | {right[:30]}")

def generate_llm_comparison(query1: str, query2: str, results1: List[Dict], results2: List[Dict]) -> str:
    """Generate AI-powered comparison between two queries"""
    try:
        context1 = prepare_context_for_llm(query1, results1[:3])
        context2 = prepare_context_for_llm(query2, results2[:3])
        
        comparison_prompt = f'''You are analyzing and comparing two different food preference queries. Please provide a thoughtful comparison.

    Query 1: "{query1}"
    Top Results for Query 1:
    {context1}

    Query 2: "{query2}"
    Top Results for Query 2:
    {context2}

    Please provide a short comparison that:
    1. Highlights the key differences between these two food preferences
    2. Notes any similarities or overlaps
    3. Explains which query might be better for different situations
    4. Recommends the best option from each query
    5. Keeps the analysis concise but insightful

    Comparison:'''

        return generate_response(comparison_prompt)
            
    except Exception as e:
        return generate_simple_comparison(query1, query2, results1, results2)

def generate_simple_comparison(query1: str, query2: str, results1: List[Dict], results2: List[Dict]) -> str:
    """Simple comparison fallback"""
    if not results1 and not results2:
        return "No results found for either query."
    if not results1:
        return f"Found results for '{query2}' but none for '{query1}'."
    if not results2:
        return f"Found results for '{query1}' but none for '{query2}'."
    
    return f"For '{query1}', I recommend {results1[0]['food_name']}. For '{query2}', {results2[0]['food_name']} would be perfect."

def show_help():
    """Display help information for RAG chatbot"""
    print("\nğŸ“– RAG CHATBOT HELP")
    print("=" * 45)
    print("ğŸ§  This chatbot uses Anthropic Claude to understand your")
    print("   food preferences and provide intelligent recommendations.")
    print("\nHow to get the best recommendations:")
    print("  â€¢ Be specific: 'healthy Italian pasta under 350 calories'")
    print("  â€¢ Mention preferences: 'spicy comfort food for cold weather'")
    print("  â€¢ Include context: 'light breakfast for busy morning'")
    print("  â€¢ Ask about benefits: 'protein-rich foods for workout recovery'")
    print("\nSpecial features:")
    print("  â€¢ ğŸ” Vector similarity search finds relevant foods")
    print("  â€¢ ğŸ§  AI analysis provides contextual explanations")
    print("  â€¢ ğŸ“Š Detailed nutritional and cuisine information")
    print("  â€¢ ğŸ”„ Smart comparison between different preferences")
    print("\nCommands:")
    print("  â€¢ 'compare' - AI-powered comparison of two queries")
    print("  â€¢ 'help' - Show this help menu")
    print("  â€¢ 'quit' - Exit the chatbot")
    print("\nTips for better results:")
    print("  â€¢ Use natural language - talk like you would to a friend")
    print("  â€¢ Mention dietary restrictions or preferences")
    print("  â€¢ Include meal timing (breakfast, lunch, dinner)")
    print("  â€¢ Specify if you want healthy, comfort, or indulgent options")


def main():
    """Main function for enhanced RAG chatbot system"""
    try:
        print("ğŸ¤– Enhanced RAG-Powered Food Recommendation Chatbot")
        print("   Powered by Anthropic Claude & ChromaDB")
        print("=" * 55)
        
        # Create collection and load food data
        print("ğŸ“¦ Loading food data and creating vector database...")
        collection = initiate_food_collection()
        print("âœ… Vector database ready")
        
        # Test LLM connection
        print("ğŸ”— Testing LLM connection...")
        test_response = generate_response("Hello")
        if test_response:
            print("âœ… LLM connection established")
        else:
            print("âŒ LLM connection failed")
            return
        
        # Start RAG chatbot
        start_chat(collection)
        
    except Exception as error:
        print(f"âŒ Error: {error}")


if __name__ == "__main__":
    main()
